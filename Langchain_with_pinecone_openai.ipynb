{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1KsKd7nt+UxJK+7YkcKg3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avikumart/LLM-GenAI-Transformers-Notebooks/blob/main/Langchain_with_pinecone_openai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom question-answering using LangChain and Pinecone"
      ],
      "metadata": {
        "id": "GG4HvbTTySUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Advantages of custom q&a over a fine-tuning**\n",
        "\n",
        "1. Context specific answers\n",
        "2. Adaptable to new input documents\n",
        "3. No need to fine-tune model which saves the cost of model training\n",
        "4. More accurate and specific answers rather than general answers\n",
        "\n",
        "\n",
        "- **LangChain modules**\n",
        "\n",
        "1. Models: to access the LLMs\n",
        "2. Indexes: to store the documents in vector database\n",
        "3. Chains: to perform specific tasks in a sequential manner"
      ],
      "metadata": {
        "id": "9FYHWTkW5gFv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9emDmRWox23J"
      },
      "outputs": [],
      "source": [
        "# install langchain and openai with other dependencies\n",
        "!pip install --upgrade langchain openai -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pillow==6.2.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "ELH7tByEDLtF",
        "outputId": "911d4988-2f29-4b7b-c718-4f3e87adbc3a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow==6.2.2\n",
            "  Using cached Pillow-6.2.2-cp310-cp310-linux_x86_64.whl\n",
            "Installing collected packages: pillow\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 10.0.0\n",
            "    Uninstalling Pillow-10.0.0:\n",
            "      Successfully uninstalled Pillow-10.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bokeh 2.4.3 requires pillow>=7.1.0, but you have pillow 6.2.2 which is incompatible.\n",
            "detectron2 0.6 requires Pillow>=7.1, but you have pillow 6.2.2 which is incompatible.\n",
            "dopamine-rl 4.0.6 requires Pillow>=7.0.0, but you have pillow 6.2.2 which is incompatible.\n",
            "imageio 2.25.1 requires pillow>=8.3.2, but you have pillow 6.2.2 which is incompatible.\n",
            "pdfplumber 0.10.0 requires Pillow>=9.1, but you have pillow 6.2.2 which is incompatible.\n",
            "pytesseract 0.3.10 requires Pillow>=8.0.0, but you have pillow 6.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pillow-6.2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured -q\n",
        "!pip install unstructured[local-inference] -q\n",
        "!pip install detectron2@git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2 -q\n",
        "!apt-get install poppler-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYyhnr9s7aGe",
        "outputId": "d46f27cd-81a9-41a2-cb75-a7f983cf570e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (0.86.1-0ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pinecone-client -q"
      ],
      "metadata": {
        "id": "CWw6Wxyt9CmW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken -q"
      ],
      "metadata": {
        "id": "eNGAZF6h9OkP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "cPr_-UZQ7jeD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import os\n",
        "import openai\n",
        "import pinecone\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEPGMrmR82gn",
        "outputId": "1508e37e-047d-4e6a-e232-e4f364825cb6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document loader"
      ],
      "metadata": {
        "id": "i0hv4l7CKSFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the documents\n",
        "directory = '/content/pets'\n",
        "\n",
        "def load_docs(directory):\n",
        "  loader = DirectoryLoader(directory)\n",
        "  documents = loader.load()\n",
        "  return documents\n",
        "\n",
        "documents = load_docs(directory)\n",
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP2iyRuF9GFi",
        "outputId": "0659d227-26ad-40cd-d154-2548f4f6ec63"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recursive text splitter"
      ],
      "metadata": {
        "id": "LgxuPWfVKPAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the docs\n",
        "def split_docs(documents, chunk_size=200, chunk_overlap=20):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "  docs = text_splitter.split_documents(documents)\n",
        "  return docs\n",
        "\n",
        "docs = split_docs(documents)\n",
        "print(len(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nkfH2koFVvj",
        "outputId": "6b02d916-c301-4dc7-f13d-5f2e2718ea23"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI embeddings"
      ],
      "metadata": {
        "id": "ozYEt25sKLWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding example on random word\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "query_result = embeddings.embed_query(\"Hello world! how are you?\")\n",
        "len(query_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b22cbxJHJ2LD",
        "outputId": "e5928b15-cb2f-4c77-f280-b30154da4e8b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pinecone client initialization"
      ],
      "metadata": {
        "id": "k_Z3GpL1L6pb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RlOebYwcK3MU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}